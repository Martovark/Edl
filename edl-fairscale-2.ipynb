{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fairscale","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:49:09.413222Z","iopub.execute_input":"2023-03-26T20:49:09.414317Z","iopub.status.idle":"2023-03-26T20:49:44.208313Z","shell.execute_reply.started":"2023-03-26T20:49:09.414259Z","shell.execute_reply":"2023-03-26T20:49:44.206848Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting fairscale\n  Downloading fairscale-0.4.6.tar.gz (248 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from fairscale) (1.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.8.0->fairscale) (4.4.0)\nBuilding wheels for collected packages: fairscale\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307224 sha256=eef3381374e19d0b4346d5b324cb47f15faf3409ba6bf91ccc5c4ffae63ff087\n  Stored in directory: /root/.cache/pip/wheels/0b/8c/fa/a9e102632bcb86e919561cf25ca1e0dd2ec67476f3a5544653\nSuccessfully built fairscale\nInstalling collected packages: fairscale\nSuccessfully installed fairscale-0.4.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile model.py\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torch.distributed as dist\n\nimport os\nimport torch\nimport torchvision\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(400, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\ndef get_loader():\n    transform = transforms.Compose(\n        [transforms.ToTensor(),\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    \n    batch_size = 64\n    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\n    dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                              shuffle=False, num_workers=2)\n    return dataloader\n\n\ndef init_process(rank, size, epochs, fn, backend='nccl'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size, epochs)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:51:50.802004Z","iopub.execute_input":"2023-03-26T20:51:50.803054Z","iopub.status.idle":"2023-03-26T20:51:50.812036Z","shell.execute_reply.started":"2023-03-26T20:51:50.802990Z","shell.execute_reply":"2023-03-26T20:51:50.810711Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Overwriting model.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 1. OSS (Optimizer state sharding)","metadata":{}},{"cell_type":"code","source":"%%writefile oss.py\n\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torchvision\nimport torch.distributed as dist\nimport torchvision.transforms as transforms\nimport torch.multiprocessing as mp\n\nfrom fairscale.optim.oss import OSS\nfrom fairscale.nn.data_parallel import ShardedDataParallel as ShardedDDP\n\nfrom functools import partial\nfrom tqdm import tqdm\n\nfrom model import Net, get_loader, init_process\n    \ndef train(\n    rank: int,\n    world_size: int,\n    epochs: int):\n    \n    dataloader = get_loader()\n    \n    device = torch.device(f'cuda:{rank}')\n    model = Net().to(device)\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    # optimizer specific arguments e.g. LR, momentum, etc...\n    base_optimizer_arguments = {\"lr\": 1e-4}\n\n    # Wrap a base optimizer into OSS\n    base_optimizer = torch.optim.SGD  # any pytorch compliant optimizer\n    optimizer = OSS(\n        params=model.parameters(),\n        optim=base_optimizer,\n        **base_optimizer_arguments)\n\n    # Wrap the model into ShardedDDP, which will reduce gradients to the proper ranks\n    model = ShardedDDP(model, optimizer)\n\n    print('-' * 50)\n    print(f'Optim params in rank: {rank}')\n    for elem in optimizer.partition_parameters()[rank]:\n        for param in elem['params']:\n            print(f'shape: {param.shape}')\n    print('-' * 50)\n    \n    model.train()\n    for e in range(epochs):\n        for idx, (data, target) in enumerate(tqdm(dataloader)):\n            data, target = data.to(device), target.to(device)\n            # new\n            model.zero_grad()\n            outputs = model(data)\n            loss = loss_fn(outputs, target)\n            loss.backward()\n            optimizer.step()\n                                \nsize, epochs = 2, 1\n\nif __name__ == '__main__':\n    fn = partial(init_process, size=size, epochs=epochs, fn=train, backend='nccl')\n    mp.spawn(\n            fn,\n            nprocs=size,\n            join=True\n        )","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:51:55.683234Z","iopub.execute_input":"2023-03-26T20:51:55.683590Z","iopub.status.idle":"2023-03-26T20:51:55.692003Z","shell.execute_reply.started":"2023-03-26T20:51:55.683557Z","shell.execute_reply":"2023-03-26T20:51:55.690561Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Overwriting oss.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### We can see that the optimizer has split the parameters between workers.","metadata":{}},{"cell_type":"code","source":"# self.conv1 = nn.Conv2d(3, 6, 5)\n# self.pool = nn.MaxPool2d(2, 2)\n# self.conv2 = nn.Conv2d(6, 16, 5)\n# self.fc1 = nn.Linear(400, 120)\n# self.fc2 = nn.Linear(120, 84)\n# self.fc3 = nn.Linear(84, 10)\n!python3 oss.py","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:52:00.068555Z","iopub.execute_input":"2023-03-26T20:52:00.068918Z","iopub.status.idle":"2023-03-26T20:52:28.058568Z","shell.execute_reply.started":"2023-03-26T20:52:00.068883Z","shell.execute_reply":"2023-03-26T20:52:28.057106Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n--------------------------------------------------\n--------------------------------------------------\nOptim params in rank: 0\nshape: torch.Size([6, 3, 5, 5])\nshape: torch.Size([16])\nshape: torch.Size([120, 400])\n--------------------------------------------------\nOptim params in rank: 1\nshape: torch.Size([6])\nshape: torch.Size([16, 6, 5, 5])\nshape: torch.Size([120])\nshape: torch.Size([84, 120])\nshape: torch.Size([84])\nshape: torch.Size([10, 84])\nshape: torch.Size([10])\n--------------------------------------------------\n100%|█████████████████████████████████████████| 157/157 [00:16<00:00,  9.69it/s]\n100%|█████████████████████████████████████████| 157/157 [00:16<00:00,  9.70it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 2. FSDP (Fully Sharded Data Parallel)\nInteresting params:\n* mixed_precision\n* move_params_to_cpu\n* move_grads_to_cpu\n\nYou can use mixed_precision, but with special scaler ShardedGradScaler.","metadata":{}},{"cell_type":"code","source":"%%writefile fsdp.py\n\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torchvision\nimport torch.distributed as dist\nimport torchvision.transforms as transforms\nimport torch.multiprocessing as mp\n\nfrom fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\nfrom fairscale.optim.grad_scaler import ShardedGradScaler\n\n\nfrom functools import partial\nfrom tqdm import tqdm\n\nfrom model import Net, get_loader, init_process\n\nimport warnings\nwarnings.filterwarnings('ignore')\n    \ndef train(\n    rank: int,\n    world_size: int,\n    epochs: int):\n    \n    torch.cuda.set_device(rank)\n    \n    dataloader = get_loader()\n    model = Net()\n    base_optimizer_arguments = {\"lr\": 1e-4}\n    \n    model = FSDP(\n        model,\n        mixed_precision=True,\n        reshard_after_forward=True,\n        move_params_to_cpu=True,\n        move_grads_to_cpu=True\n    )\n    \n    optimizer = torch.optim.SGD(\n        params=model.parameters(),\n        **base_optimizer_arguments\n    )\n    \n    loss_fn = torch.nn.CrossEntropyLoss()\n    scaler = ShardedGradScaler()\n    \n    # uncomment if move_params_to_cpu=False\n    # model = model.to(rank)\n    \n    model.train()\n    for e in range(epochs):\n        for idx, (data, target) in enumerate(tqdm(dataloader)):\n            data, target = data.to(rank), target.to(rank)\n            model.zero_grad(set_to_none=True)\n            \n            with torch.autocast(device_type='cuda'):\n                outputs = model(data)\n                loss = loss_fn(outputs, target)\n                \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            if idx == 0:\n                if rank == 1:\n                    dist.barrier()\n                print('-' * 50)\n                print(f'rank: {rank}')\n                print(f'Param after backward')\n                for param in optimizer.param_groups[0]['params']:\n                    print(f'Shape: {param.shape}')\n                    print(param)\n                print('-' * 50)\n                if rank == 0:\n                    dist.barrier()\n                                \nsize, epochs = 2, 1\n\nif __name__ == '__main__':\n    fn = partial(init_process, size=size, epochs=epochs, fn=train, backend='nccl')\n    mp.spawn(\n            fn,\n            nprocs=size,\n            join=True\n        )","metadata":{"execution":{"iopub.status.busy":"2023-03-26T18:28:49.152667Z","iopub.execute_input":"2023-03-26T18:28:49.153131Z","iopub.status.idle":"2023-03-26T18:28:49.162220Z","shell.execute_reply.started":"2023-03-26T18:28:49.153089Z","shell.execute_reply":"2023-03-26T18:28:49.161075Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Overwriting fsdp.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Number of parameters: 62006, at each gpu: 31003","metadata":{}},{"cell_type":"code","source":"!python3 fsdp.py","metadata":{"execution":{"iopub.status.busy":"2023-03-26T18:28:51.768847Z","iopub.execute_input":"2023-03-26T18:28:51.769949Z","iopub.status.idle":"2023-03-26T18:29:17.732575Z","shell.execute_reply.started":"2023-03-26T18:28:51.769900Z","shell.execute_reply":"2023-03-26T18:29:17.731337Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n  0%|                                                   | 0/157 [00:00<?, ?it/s]--------------------------------------------------\nrank: 0\nParam after backward\nShape: torch.Size([31003])\nParameter containing:\nParameter(FlatParameter([ 0.0477, -0.0293,  0.0357,  ..., -0.0013,  0.0012,  0.0489],\n              requires_grad=True))\n--------------------------------------------------\n--------------------------------------------------\nrank: 1\nParam after backward\nShape: torch.Size([31003])\n  1%|▎                                          | 1/157 [00:11<30:40, 11.80s/it]Parameter containing:\nParameter(FlatParameter([-0.0184,  0.0203,  0.0362,  ..., -0.0378, -0.0753, -0.1072],\n              requires_grad=True))\n--------------------------------------------------\n100%|█████████████████████████████████████████| 157/157 [00:18<00:00,  8.27it/s]\n100%|█████████████████████████████████████████| 157/157 [00:18<00:00,  8.26it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 3. Wrap individual modules","metadata":{}},{"cell_type":"code","source":"%%writefile wrap.py\n\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torchvision\nimport torch.distributed as dist\nimport torchvision.transforms as transforms\nimport torch.multiprocessing as mp\n\nfrom fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\nfrom fairscale.nn.wrap import wrap, enable_wrap, auto_wrap\n\nfrom functools import partial\nfrom tqdm import tqdm\n\nfrom model import Net, get_loader, init_process\n    \ndef train(\n    rank: int,\n    world_size: int,\n    epochs: int):\n\n    torch.cuda.set_device(rank)\n    \n    model = Net()\n    model = model.to(rank)\n    \n    model.train()\n    model = model.to(rank)\n    fsdp_params = dict(wrapper_cls=FSDP, mixed_precision=True, flatten_parameters=True)\n    with enable_wrap(**fsdp_params):\n        \n        # auto_wrap_policy=functools.partial(default_auto_wrap_policy, \n        # min_num_params=1e3)\n        \n        model.fc3 = wrap(model.fc3)\n        if rank == 1:\n            dist.barrier()\n        print('-' * 50)\n        print(f'RANK: {rank}')\n        print(f'Wrapped: {model.fc3}')\n        print(f'Unwrapped: {model.conv2}')\n        print('-' * 50)\n        if rank == 0:\n            dist.barrier()\n                                \nsize, epochs = 2, 1\n\nif __name__ == '__main__':\n    fn = partial(init_process, size=size, epochs=epochs, fn=train, backend='nccl')\n    mp.spawn(\n            fn,\n            nprocs=size,\n            join=True\n        )","metadata":{"execution":{"iopub.status.busy":"2023-03-26T18:55:32.795424Z","iopub.execute_input":"2023-03-26T18:55:32.795856Z","iopub.status.idle":"2023-03-26T18:55:32.804861Z","shell.execute_reply.started":"2023-03-26T18:55:32.795816Z","shell.execute_reply":"2023-03-26T18:55:32.803584Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"Overwriting wrap.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python3 wrap.py","metadata":{"execution":{"iopub.status.busy":"2023-03-26T18:55:35.161700Z","iopub.execute_input":"2023-03-26T18:55:35.162650Z","iopub.status.idle":"2023-03-26T18:55:44.795145Z","shell.execute_reply.started":"2023-03-26T18:55:35.162595Z","shell.execute_reply":"2023-03-26T18:55:44.793624Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"--------------------------------------------------\nRANK: 0\nWrapped: FullyShardedDataParallel(\n  world_size=2, flatten_parameters=True, mixed_precision=True, \n  (_fsdp_wrapped_module): FlattenParamsWrapper(\n    (_fpw_module): Linear(in_features=84, out_features=10, bias=True)\n  )\n)\nUnwrapped: Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n--------------------------------------------------\n--------------------------------------------------\nRANK: 1\nWrapped: FullyShardedDataParallel(\n  world_size=2, flatten_parameters=True, mixed_precision=True, \n  (_fsdp_wrapped_module): FlattenParamsWrapper(\n    (_fpw_module): Linear(in_features=84, out_features=10, bias=True)\n  )\n)\nUnwrapped: Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n--------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"4. #### Slowmo DDP\n\nSlowMo Distributed Data Parallel reduces the communication between different nodes while performing data parallel training.","metadata":{}},{"cell_type":"code","source":"%%writefile slowmo.py\n\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torchvision\nimport torch.distributed as dist\nimport torchvision.transforms as transforms\nimport torch.multiprocessing as mp\n\nfrom fairscale.experimental.nn.data_parallel \\\n        import SlowMoDistributedDataParallel as SlowMoDDP\n\nfrom functools import partial\nfrom tqdm import tqdm\n\nfrom model import Net, get_loader, init_process\n\nimport warnings\nwarnings.filterwarnings('ignore')\n    \ndef train(\n    rank: int,\n    world_size: int,\n    epochs: int):\n    \n    torch.cuda.set_device(rank)\n    \n    dataloader = get_loader()\n    \n    model = Net().to(rank)\n    model = SlowMoDDP(model, slowmo_momentum=0.5, nprocs_per_node=2)\n    \n    base_optimizer_arguments = {\"lr\": 1e-4}\n    optimizer = torch.optim.SGD(\n        params=model.parameters(),\n        **base_optimizer_arguments\n    )\n    \n    loss_fn = torch.nn.CrossEntropyLoss()\n    \n    model.train()\n    for e in range(epochs):\n        for idx, (data, target) in enumerate(tqdm(dataloader)):\n            data, target = data.to(rank), target.to(rank)\n            # new\n            model.zero_grad(set_to_none=True)\n            outputs = model(data)\n            loss = loss_fn(outputs, target)\n            loss.backward()\n            optimizer.step()\n            \n            model.perform_slowmo(optimizer)\n                                \nsize, epochs = 2, 1\n\nif __name__ == '__main__':\n    fn = partial(init_process, size=size, epochs=epochs, fn=train, backend='nccl')\n    mp.spawn(\n            fn,\n            nprocs=size,\n            join=True\n        )","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:06:45.278586Z","iopub.execute_input":"2023-03-26T19:06:45.279197Z","iopub.status.idle":"2023-03-26T19:06:45.289487Z","shell.execute_reply.started":"2023-03-26T19:06:45.279154Z","shell.execute_reply":"2023-03-26T19:06:45.288028Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Overwriting slowmo.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python3 slowmo.py","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:06:48.139567Z","iopub.execute_input":"2023-03-26T19:06:48.139952Z","iopub.status.idle":"2023-03-26T19:07:15.021834Z","shell.execute_reply.started":"2023-03-26T19:06:48.139917Z","shell.execute_reply":"2023-03-26T19:07:15.020420Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n100%|█████████████████████████████████████████| 157/157 [00:15<00:00, 10.08it/s]\n100%|█████████████████████████████████████████| 157/157 [00:15<00:00,  9.97it/s]\n","output_type":"stream"}]}]}