{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-03-26T17:02:21.502545Z","iopub.status.busy":"2023-03-26T17:02:21.501789Z","iopub.status.idle":"2023-03-26T17:02:52.972014Z","shell.execute_reply":"2023-03-26T17:02:52.970827Z","shell.execute_reply.started":"2023-03-26T17:02:21.502508Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting fairscale\n","  Downloading fairscale-0.4.6.tar.gz (248 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from fairscale) (1.13.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.8.0->fairscale) (4.4.0)\n","Building wheels for collected packages: fairscale\n","  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307224 sha256=a3d7fe85da2bb39f237abdba3033dfd42ee12dcb2177778b1200f5afc5d14cfa\n","  Stored in directory: /root/.cache/pip/wheels/0b/8c/fa/a9e102632bcb86e919561cf25ca1e0dd2ec67476f3a5544653\n","Successfully built fairscale\n","Installing collected packages: fairscale\n","Successfully installed fairscale-0.4.6\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install fairscale"]},{"cell_type":"markdown","metadata":{},"source":["#### 1. OSS (Optimizer state sharding)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-26T17:32:57.954774Z","iopub.status.busy":"2023-03-26T17:32:57.954395Z","iopub.status.idle":"2023-03-26T17:32:57.963945Z","shell.execute_reply":"2023-03-26T17:32:57.962864Z","shell.execute_reply.started":"2023-03-26T17:32:57.954736Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting oss.py\n"]}],"source":["%%writefile oss.py\n","\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import torchvision\n","import torch.distributed as dist\n","import torchvision.transforms as transforms\n","import torch.multiprocessing as mp\n","\n","from fairscale.optim.oss import OSS\n","from fairscale.nn.data_parallel import ShardedDataParallel as ShardedDDP\n","\n","from functools import partial\n","from tqdm import tqdm\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(400, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def get_loader():\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","    \n","    batch_size = 64\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","    dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                              shuffle=False, num_workers=2)\n","    return dataloader\n","\n","def init_process(rank, size, epochs, fn, backend='nccl'):\n","    \"\"\" Initialize the distributed environment. \"\"\"\n","    os.environ['MASTER_ADDR'] = '127.0.0.1'\n","    os.environ['MASTER_PORT'] = '29500'\n","    dist.init_process_group(backend, rank=rank, world_size=size)\n","    fn(rank, size, epochs)\n","    \n","def train(\n","    rank: int,\n","    world_size: int,\n","    epochs: int):\n","    \n","    dataloader = get_loader()\n","    \n","    device = torch.device(f'cuda:{rank}')\n","    model = Net().to(device)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    # optimizer specific arguments e.g. LR, momentum, etc...\n","    base_optimizer_arguments = {\"lr\": 1e-4}\n","\n","    # Wrap a base optimizer into OSS\n","    base_optimizer = torch.optim.SGD  # any pytorch compliant optimizer\n","    optimizer = OSS(\n","        params=model.parameters(),\n","        optim=base_optimizer,\n","        **base_optimizer_arguments)\n","\n","    # Wrap the model into ShardedDDP, which will reduce gradients to the proper ranks\n","    model = ShardedDDP(model, optimizer)\n","\n","    print('-' * 50)\n","    print(f'Optim params in rank: {rank}')\n","    for elem in optimizer.partition_parameters()[rank]:\n","        for param in elem['params']:\n","            print(f'shape: {param.shape}')\n","    print('-' * 50)\n","    \n","    model.train()\n","    for e in range(epochs):\n","        for idx, (data, target) in enumerate(tqdm(dataloader)):\n","            data, target = data.to(device), target.to(device)\n","            # new\n","            model.zero_grad()\n","            outputs = model(data)\n","            loss = loss_fn(outputs, target)\n","            loss.backward()\n","            optimizer.step()\n","                                \n","size, epochs = 2, 1\n","\n","if __name__ == '__main__':\n","    fn = partial(init_process, size=size, epochs=epochs, fn=train, backend='nccl')\n","    mp.spawn(\n","            fn,\n","            nprocs=size,\n","            join=True\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["#### We can see that the optimizer has split the parameters between workers."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-03-26T17:03:12.441624Z","iopub.status.busy":"2023-03-26T17:03:12.441244Z","iopub.status.idle":"2023-03-26T17:03:50.656092Z","shell.execute_reply":"2023-03-26T17:03:50.654899Z","shell.execute_reply.started":"2023-03-26T17:03:12.441591Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","100%|███████████████████████| 170498071/170498071 [00:03<00:00, 45046121.72it/s]\n"," 89%|████████████████████▍  | 151683072/170498071 [00:04<00:00, 48235582.26it/s]Extracting ./data/cifar-10-python.tar.gz to ./data\n","100%|███████████████████████| 170498071/170498071 [00:04<00:00, 38430142.82it/s]\n","Extracting ./data/cifar-10-python.tar.gz to ./data\n","--------------------------------------------------\n","Optim params in rank :0\n","shape: torch.Size([6, 3, 5, 5])\n","shape: torch.Size([16])\n","shape: torch.Size([120, 400])\n","--------------------------------------------------\n","  0%|                                                   | 0/157 [00:00<?, ?it/s]--------------------------------------------------\n","Optim params in rank :1\n","shape: torch.Size([6])\n","shape: torch.Size([16, 6, 5, 5])\n","shape: torch.Size([120])\n","shape: torch.Size([84, 120])\n","shape: torch.Size([84])\n","shape: torch.Size([10, 84])\n","shape: torch.Size([10])\n","--------------------------------------------------\n","100%|█████████████████████████████████████████| 157/157 [00:18<00:00,  8.35it/s]\n","100%|█████████████████████████████████████████| 157/157 [00:18<00:00,  8.34it/s]\n"]}],"source":["# self.conv1 = nn.Conv2d(3, 6, 5)\n","# self.pool = nn.MaxPool2d(2, 2)\n","# self.conv2 = nn.Conv2d(6, 16, 5)\n","# self.fc1 = nn.Linear(400, 120)\n","# self.fc2 = nn.Linear(120, 84)\n","# self.fc3 = nn.Linear(84, 10)\n","!python3 oss.py"]},{"cell_type":"markdown","metadata":{},"source":["#### 2. FSDP (Fully Sharded Data Parallel)\n","Interesting params:\n","* mixed_precision\n","* move_params_to_cpu\n","* move_grads_to_cpu\n","\n","You can use mixed_precision, but with special scaler ShardedGradScaler."]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-03-26T18:28:49.153131Z","iopub.status.busy":"2023-03-26T18:28:49.152667Z","iopub.status.idle":"2023-03-26T18:28:49.162220Z","shell.execute_reply":"2023-03-26T18:28:49.161075Z","shell.execute_reply.started":"2023-03-26T18:28:49.153089Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting fsdp.py\n"]}],"source":["%%writefile fsdp.py\n","\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import torchvision\n","import torch.distributed as dist\n","import torchvision.transforms as transforms\n","import torch.multiprocessing as mp\n","\n","from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n","from fairscale.optim.grad_scaler import ShardedGradScaler\n","\n","\n","from functools import partial\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(400, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def get_loader():\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","    \n","    batch_size = 64\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","    dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                              shuffle=False, num_workers=2)\n","    return dataloader\n","\n","def init_process(rank, size, epochs, fn, backend='nccl'):\n","    \"\"\" Initialize the distributed environment. \"\"\"\n","    os.environ['MASTER_ADDR'] = '127.0.0.1'\n","    os.environ['MASTER_PORT'] = '30000'\n","    dist.init_process_group(backend, rank=rank, world_size=size)\n","    fn(rank, size, epochs)\n","    \n","def train(\n","    rank: int,\n","    world_size: int,\n","    epochs: int):\n","    \n","    torch.cuda.set_device(rank)\n","    \n","    dataloader = get_loader()\n","    model = Net()\n","    base_optimizer_arguments = {\"lr\": 1e-4}\n","    \n","    model = FSDP(\n","        model,\n","        mixed_precision=True,\n","        reshard_after_forward=True,\n","        move_params_to_cpu=True,\n","        move_grads_to_cpu=True\n","    )\n","    \n","    optimizer = torch.optim.SGD(\n","        params=model.parameters(),\n","        **base_optimizer_arguments\n","    )\n","    \n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    scaler = ShardedGradScaler()\n","    \n","    # uncomment if move_params_to_cpu=False\n","    # model = model.to(rank)\n","    \n","    model.train()\n","    for e in range(epochs):\n","        for idx, (data, target) in enumerate(tqdm(dataloader)):\n","            data, target = data.to(rank), target.to(rank)\n","            model.zero_grad(set_to_none=True)\n","            \n","            with torch.autocast(device_type='cuda'):\n","                outputs = model(data)\n","                loss = loss_fn(outputs, target)\n","                \n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            if idx == 0:\n","                if rank == 1:\n","                    dist.barrier()\n","                print('-' * 50)\n","                print(f'rank: {rank}')\n","                print(f'Param after backward')\n","                for param in optimizer.param_groups[0]['params']:\n","                    print(f'Shape: {param.shape}')\n","                    print(param)\n","                print('-' * 50)\n","                if rank == 0:\n","                    dist.barrier()\n","                                \n","size, epochs = 2, 1\n","\n","if __name__ == '__main__':\n","    fn = partial(init_process, size=size, epochs=epochs, fn=train, backend='nccl')\n","    mp.spawn(\n","            fn,\n","            nprocs=size,\n","            join=True\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["#### Number of parameters: 62006, at each gpu: 31003"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-03-26T18:28:51.769949Z","iopub.status.busy":"2023-03-26T18:28:51.768847Z","iopub.status.idle":"2023-03-26T18:29:17.732575Z","shell.execute_reply":"2023-03-26T18:29:17.731337Z","shell.execute_reply.started":"2023-03-26T18:28:51.769900Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","  0%|                                                   | 0/157 [00:00<?, ?it/s]--------------------------------------------------\n","rank: 0\n","Param after backward\n","Shape: torch.Size([31003])\n","Parameter containing:\n","Parameter(FlatParameter([ 0.0477, -0.0293,  0.0357,  ..., -0.0013,  0.0012,  0.0489],\n","              requires_grad=True))\n","--------------------------------------------------\n","--------------------------------------------------\n","rank: 1\n","Param after backward\n","Shape: torch.Size([31003])\n","  1%|▎                                          | 1/157 [00:11<30:40, 11.80s/it]Parameter containing:\n","Parameter(FlatParameter([-0.0184,  0.0203,  0.0362,  ..., -0.0378, -0.0753, -0.1072],\n","              requires_grad=True))\n","--------------------------------------------------\n","100%|█████████████████████████████████████████| 157/157 [00:18<00:00,  8.27it/s]\n","100%|█████████████████████████████████████████| 157/157 [00:18<00:00,  8.26it/s]\n"]}],"source":["!python3 fsdp.py"]},{"cell_type":"markdown","metadata":{},"source":["#### 3. Wrap individual modules"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2023-03-26T18:55:32.795856Z","iopub.status.busy":"2023-03-26T18:55:32.795424Z","iopub.status.idle":"2023-03-26T18:55:32.804861Z","shell.execute_reply":"2023-03-26T18:55:32.803584Z","shell.execute_reply.started":"2023-03-26T18:55:32.795816Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting wrap.py\n"]}],"source":["%%writefile wrap.py\n","\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import torchvision\n","import torch.distributed as dist\n","import torchvision.transforms as transforms\n","import torch.multiprocessing as mp\n","\n","from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n","from fairscale.nn.wrap import wrap, enable_wrap, auto_wrap\n","\n","from functools import partial\n","from tqdm import tqdm\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(400, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def get_loader():\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","    \n","    batch_size = 64\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","    dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                              shuffle=False, num_workers=2)\n","    return dataloader\n","\n","def init_process(rank, size, epochs, fn, backend='nccl'):\n","    \"\"\" Initialize the distributed environment. \"\"\"\n","    os.environ['MASTER_ADDR'] = '127.0.0.1'\n","    os.environ['MASTER_PORT'] = '29500'\n","    dist.init_process_group(backend, rank=rank, world_size=size)\n","    fn(rank, size, epochs)\n","    \n","def train(\n","    rank: int,\n","    world_size: int,\n","    epochs: int):\n","\n","    torch.cuda.set_device(rank)\n","    \n","    model = Net()\n","    model = model.to(rank)\n","    \n","    model.train()\n","    model = model.to(rank)\n","    fsdp_params = dict(wrapper_cls=FSDP, mixed_precision=True, flatten_parameters=True)\n","    with enable_wrap(**fsdp_params):\n","        \n","        # auto_wrap_policy=functools.partial(default_auto_wrap_policy, \n","        # min_num_params=1e3)\n","        \n","        model.fc3 = wrap(model.fc3)\n","        if rank == 1:\n","            dist.barrier()\n","        print('-' * 50)\n","        print(f'RANK: {rank}')\n","        print(f'Wrapped: {model.fc3}')\n","        print(f'Unwrapped: {model.conv2}')\n","        print('-' * 50)\n","        if rank == 0:\n","            dist.barrier()\n","                                \n","size, epochs = 2, 1\n","\n","if __name__ == '__main__':\n","    fn = partial(init_process, size=size, epochs=epochs, fn=train, backend='nccl')\n","    mp.spawn(\n","            fn,\n","            nprocs=size,\n","            join=True\n","        )"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2023-03-26T18:55:35.162650Z","iopub.status.busy":"2023-03-26T18:55:35.161700Z","iopub.status.idle":"2023-03-26T18:55:44.795145Z","shell.execute_reply":"2023-03-26T18:55:44.793624Z","shell.execute_reply.started":"2023-03-26T18:55:35.162595Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","RANK: 0\n","Wrapped: FullyShardedDataParallel(\n","  world_size=2, flatten_parameters=True, mixed_precision=True, \n","  (_fsdp_wrapped_module): FlattenParamsWrapper(\n","    (_fpw_module): Linear(in_features=84, out_features=10, bias=True)\n","  )\n",")\n","Unwrapped: Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","--------------------------------------------------\n","--------------------------------------------------\n","RANK: 1\n","Wrapped: FullyShardedDataParallel(\n","  world_size=2, flatten_parameters=True, mixed_precision=True, \n","  (_fsdp_wrapped_module): FlattenParamsWrapper(\n","    (_fpw_module): Linear(in_features=84, out_features=10, bias=True)\n","  )\n",")\n","Unwrapped: Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","--------------------------------------------------\n"]}],"source":["!python3 wrap.py"]},{"cell_type":"markdown","metadata":{},"source":["4. #### Slowmo DDP\n","\n","SlowMo Distributed Data Parallel reduces the communication between different nodes while performing data parallel training."]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2023-03-26T19:06:45.279197Z","iopub.status.busy":"2023-03-26T19:06:45.278586Z","iopub.status.idle":"2023-03-26T19:06:45.289487Z","shell.execute_reply":"2023-03-26T19:06:45.288028Z","shell.execute_reply.started":"2023-03-26T19:06:45.279154Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting slowmo.py\n"]}],"source":["%%writefile slowmo.py\n","\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import torchvision\n","import torch.distributed as dist\n","import torchvision.transforms as transforms\n","import torch.multiprocessing as mp\n","\n","from fairscale.experimental.nn.data_parallel \\\n","        import SlowMoDistributedDataParallel as SlowMoDDP\n","\n","from functools import partial\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(400, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def get_loader():\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","    \n","    batch_size = 64\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","    dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                              shuffle=False, num_workers=2)\n","    return dataloader\n","\n","def init_process(rank, size, epochs, fn, backend='nccl'):\n","    \"\"\" Initialize the distributed environment. \"\"\"\n","    os.environ['MASTER_ADDR'] = '127.0.0.1'\n","    os.environ['MASTER_PORT'] = '29500'\n","    dist.init_process_group(backend, rank=rank, world_size=size)\n","    fn(rank, size, epochs)\n","    \n","def train(\n","    rank: int,\n","    world_size: int,\n","    epochs: int):\n","    \n","    torch.cuda.set_device(rank)\n","    \n","    dataloader = get_loader()\n","    \n","    model = Net().to(rank)\n","    model = SlowMoDDP(model, slowmo_momentum=0.5, nprocs_per_node=2)\n","    \n","    base_optimizer_arguments = {\"lr\": 1e-4}\n","    optimizer = torch.optim.SGD(\n","        params=model.parameters(),\n","        **base_optimizer_arguments\n","    )\n","    \n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    \n","    model.train()\n","    for e in range(epochs):\n","        for idx, (data, target) in enumerate(tqdm(dataloader)):\n","            data, target = data.to(rank), target.to(rank)\n","            # new\n","            model.zero_grad(set_to_none=True)\n","            outputs = model(data)\n","            loss = loss_fn(outputs, target)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            model.perform_slowmo(optimizer)\n","                                \n","size, epochs = 2, 1\n","\n","if __name__ == '__main__':\n","    fn = partial(init_process, size=size, epochs=epochs, fn=train, backend='nccl')\n","    mp.spawn(\n","            fn,\n","            nprocs=size,\n","            join=True\n","        )"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2023-03-26T19:06:48.139952Z","iopub.status.busy":"2023-03-26T19:06:48.139567Z","iopub.status.idle":"2023-03-26T19:07:15.021834Z","shell.execute_reply":"2023-03-26T19:07:15.020420Z","shell.execute_reply.started":"2023-03-26T19:06:48.139917Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","100%|█████████████████████████████████████████| 157/157 [00:15<00:00, 10.08it/s]\n","100%|█████████████████████████████████████████| 157/157 [00:15<00:00,  9.97it/s]\n"]}],"source":["!python3 slowmo.py"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
